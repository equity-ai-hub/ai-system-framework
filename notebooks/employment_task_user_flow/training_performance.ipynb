{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get performance and fairness scores of all trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the columns names - scores list\n",
    "import numpy as np\n",
    "\n",
    "temp = \"../../src/data/artifacts/acs_income/LogisticRegression/fold_0/LogisticRegression_scores.npy\"\n",
    "scores = np.load(temp, allow_pickle=True).item()\n",
    "scores_names = list(scores[\"scores\"].keys())\n",
    "generic_metrics_names = list(scores[\"generic_metrics\"].keys())\n",
    "generic_metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "artifacts_dir = \"../../src/data/artifacts/acs_income\"\n",
    "df_scores = pd.DataFrame(columns=[\"model_name\", \"kfold\", *scores_names])\n",
    "df_generic_metrics = pd.DataFrame(columns=[\"model_name\", \"kfold\", *generic_metrics_names])\n",
    "\n",
    "for model in os.listdir(artifacts_dir):\n",
    "    model_dir = os.path.join(artifacts_dir, model)\n",
    "    for kfolds in os.listdir(model_dir):\n",
    "        kfold_path = os.path.join(model_dir, kfolds)\n",
    "        for files in os.listdir(kfold_path):\n",
    "            file_path = os.path.join(kfold_path, files)\n",
    "            if file_path.endswith(\"_scores.npy\"):\n",
    "                res = np.load(file_path, allow_pickle=True).item()\n",
    "\n",
    "                scores = res[\"scores\"]\n",
    "                data_scores = {\n",
    "                    \"model_name\": res[\"model_name\"],\n",
    "                    \"kfold\": res[\"kfold\"],\n",
    "                    **scores,\n",
    "                }\n",
    "                df_scores.loc[len(df_scores)] = pd.Series(data_scores)\n",
    "\n",
    "                generic_metrics = res[\"generic_metrics\"]\n",
    "                data_generic_metrics = {\n",
    "                    \"model_name\": res[\"model_name\"],\n",
    "                    \"kfold\": res[\"kfold\"],\n",
    "                    **generic_metrics,\n",
    "                }\n",
    "                df_generic_metrics.loc[len(df_generic_metrics)] = pd.Series(data_generic_metrics)\n",
    "\n",
    "df_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_generic_metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get performance metrics values of all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confidence_interval(scores):\n",
    "    from scipy import stats\n",
    "\n",
    "    mean = scores.mean()\n",
    "    sem = stats.sem(scores)\n",
    "    ci = stats.t.interval(0.95, len(scores) - 1, loc=mean, scale=sem)\n",
    "    return ci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Scores Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selectt the metric to see the values\n",
    "metric = \"BAL_ACC\"\n",
    "\n",
    "scores = df_scores.groupby([\"model_name\"])[[metric]]\n",
    "means, stds, zscores, accs, upper_ci, lower_ci, models = [], [], [], [], [], [], []\n",
    "\n",
    "for model in scores.groups.keys():\n",
    "    models.append(model)\n",
    "    mean_acc = scores.get_group(model)[metric].mean()\n",
    "    std_acc = scores.get_group(model)[metric].std(ddof=1)\n",
    "\n",
    "    # means and stds\n",
    "    means.append(mean_acc)\n",
    "    stds.append(std_acc)\n",
    "\n",
    "    accs = scores.get_group(model)[metric]\n",
    "    ci = get_confidence_interval(accs)\n",
    "    lower_ci.append(np.abs(mean_acc - ci[0]))\n",
    "    upper_ci.append(np.abs(mean_acc - ci[1]))\n",
    "\n",
    "# create a dataframe with the results lists\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"model\": models,\n",
    "        \"metric\": metric,\n",
    "        \"mean\": means,\n",
    "        \"std\": stds,\n",
    "        \"lower_ci\": lower_ci,\n",
    "        \"upper_ci\": upper_ci,\n",
    "    }\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## p-value BAL_ACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# T-test (pairwise comparison between two models)\n",
    "acc_0 = scores.get_group(\"DecisionTreeClassifier\")[\"BAL_ACC\"].tolist()\n",
    "acc_1 = scores.get_group(\"LogisticRegression\")[\"BAL_ACC\"].tolist()\n",
    "acc_2 = scores.get_group(\"RandomForestClassifier\")[\"BAL_ACC\"].tolist()\n",
    "acc_3 = scores.get_group(\"XGBClassifier\")[\"BAL_ACC\"].tolist()\n",
    "acc_4 = scores.get_group(\"MLPClassifier\")[\"BAL_ACC\"].tolist()\n",
    "\n",
    "U1, p = mannwhitneyu(acc_0, acc_1, method=\"exact\")\n",
    "print(f\"U1: {U1}, p-value: {p}\")\n",
    "\n",
    "U2, p = mannwhitneyu(acc_0, acc_2, method=\"exact\")\n",
    "print(f\"U2: {U2}, p-value: {p}\")\n",
    "\n",
    "U3, p = mannwhitneyu(acc_0, acc_3, method=\"exact\")\n",
    "print(f\"U3: {U3}, p-value: {p}\")\n",
    "\n",
    "U4, p = mannwhitneyu(acc_0, acc_4, method=\"exact\")\n",
    "print(f\"U4: {U4}, p-value: {p}\")\n",
    "\n",
    "U5, p = mannwhitneyu(acc_1, acc_2, method=\"exact\")\n",
    "print(f\"U5: {U5}, p-value: {p}\")\n",
    "\n",
    "U6, p = mannwhitneyu(acc_1, acc_3, method=\"exact\")\n",
    "print(f\"U6: {U6}, p-value: {p}\")\n",
    "\n",
    "U7, p = mannwhitneyu(acc_1, acc_4, method=\"exact\")\n",
    "print(f\"U7: {U7}, p-value: {p}\")\n",
    "\n",
    "U8, p = mannwhitneyu(acc_2, acc_3, method=\"exact\")\n",
    "print(f\"U8: {U8}, p-value: {p}\")\n",
    "\n",
    "U9, p = mannwhitneyu(acc_2, acc_4, method=\"exact\")\n",
    "print(f\"U9: {U9}, p-value: {p}\")\n",
    "\n",
    "U10, p = mannwhitneyu(acc_3, acc_4, method=\"exact\")\n",
    "print(f\"U10: {U10}, p-value: {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, p = mannwhitneyu(acc_3, acc_0, method=\"exact\")\n",
    "print(f\"XGBClassifier x DecisionTreeClassifier p-value: {p.round(5)}\")\n",
    "\n",
    "_, p = mannwhitneyu(acc_3, acc_1, method=\"exact\")\n",
    "print(f\"XGBClassifier x LogisticRegression p-value: {p.round(5)}\")\n",
    "\n",
    "_, p = mannwhitneyu(acc_3, acc_2, method=\"exact\")\n",
    "print(f\"XGBClassifier x RandomForestClassifier p-value: {p.round(5)}\")\n",
    "\n",
    "_, p = mannwhitneyu(acc_3, acc_4, method=\"exact\")\n",
    "print(f\"XGBClassifier x MLPClassifier p-value: {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## p-value PPV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "# T-test (pairwise comparison between two models)\n",
    "scores = df_scores.groupby([\"model_name\"])[[\"PPV\"]]\n",
    "ppv_0 = scores.get_group(\"DecisionTreeClassifier\")[\"PPV\"].tolist()\n",
    "ppv_1 = scores.get_group(\"LogisticRegression\")[\"PPV\"].tolist()\n",
    "ppv_2 = scores.get_group(\"RandomForestClassifier\")[\"PPV\"].tolist()\n",
    "ppv_3 = scores.get_group(\"XGBClassifier\")[\"PPV\"].tolist()\n",
    "ppv_4 = scores.get_group(\"MLPClassifier\")[\"PPV\"].tolist()\n",
    "\n",
    "U1, p = mannwhitneyu(ppv_0, ppv_1, method=\"exact\")\n",
    "print(f\"U1: {U1}, p-value: {p}\")\n",
    "\n",
    "U2, p = mannwhitneyu(ppv_0, ppv_2, method=\"exact\")\n",
    "print(f\"U2: {U2}, p-value: {p}\")\n",
    "\n",
    "U3, p = mannwhitneyu(ppv_0, ppv_3, method=\"exact\")\n",
    "print(f\"U3: {U3}, p-value: {p}\")\n",
    "\n",
    "U4, p = mannwhitneyu(ppv_0, ppv_4, method=\"exact\")\n",
    "print(f\"U4: {U4}, p-value: {p}\")\n",
    "\n",
    "U5, p = mannwhitneyu(ppv_1, ppv_2, method=\"exact\")\n",
    "print(f\"U5: {U5}, p-value: {p}\")\n",
    "\n",
    "U6, p = mannwhitneyu(ppv_1, ppv_3, method=\"exact\")\n",
    "print(f\"U6: {U6}, p-value: {p}\")\n",
    "\n",
    "U7, p = mannwhitneyu(ppv_1, ppv_4, method=\"exact\")\n",
    "print(f\"U7: {U7}, p-value: {p}\")\n",
    "\n",
    "U8, p = mannwhitneyu(ppv_2, ppv_3, method=\"exact\")\n",
    "print(f\"U8: {U8}, p-value: {p}\")\n",
    "\n",
    "U9, p = mannwhitneyu(ppv_2, ppv_4, method=\"exact\")\n",
    "print(f\"U9: {U9}, p-value: {p}\")\n",
    "\n",
    "U10, p = mannwhitneyu(ppv_3, ppv_4, method=\"exact\")\n",
    "print(f\"U10: {U10}, p-value: {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = df_scores.groupby([\"model_name\"])[[\"PPV\"]]\n",
    "ppv_0 = scores.get_group(\"DecisionTreeClassifier\")[\"PPV\"].tolist()\n",
    "ppv_1 = scores.get_group(\"LogisticRegression\")[\"PPV\"].tolist()\n",
    "ppv_2 = scores.get_group(\"RandomForestClassifier\")[\"PPV\"].tolist()\n",
    "ppv_3 = scores.get_group(\"XGBClassifier\")[\"PPV\"].tolist()\n",
    "ppv_4 = scores.get_group(\"MLPClassifier\")[\"PPV\"].tolist()\n",
    "\n",
    "_, p = mannwhitneyu(ppv_3, ppv_0, method=\"exact\")\n",
    "print(f\"XGBClassifier x DecisionTreeClassifier p-value: {p.round(5)}\")\n",
    "\n",
    "_, p = mannwhitneyu(ppv_3, ppv_1, method=\"exact\")\n",
    "print(f\"XGBClassifier x LogisticRegression p-value: {p.round(5)}\")\n",
    "\n",
    "_, p = mannwhitneyu(ppv_3, ppv_2, method=\"exact\")\n",
    "print(f\"XGBClassifier x RandomForestClassifier p-value: {p.round(5)}\")\n",
    "\n",
    "_, p = mannwhitneyu(ppv_3, ppv_4, method=\"exact\")\n",
    "print(f\"XGBClassifier x MLPClassifier p-value: {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## p-value TPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = df_scores.groupby([\"model_name\"])[[\"TPR\"]]\n",
    "TPR_0 = scores.get_group(\"DecisionTreeClassifier\")[\"TPR\"].tolist()\n",
    "TPR_1 = scores.get_group(\"LogisticRegression\")[\"TPR\"].tolist()\n",
    "TPR_2 = scores.get_group(\"RandomForestClassifier\")[\"TPR\"].tolist()\n",
    "TPR_3 = scores.get_group(\"XGBClassifier\")[\"TPR\"].tolist()\n",
    "TPR_4 = scores.get_group(\"MLPClassifier\")[\"TPR\"].tolist()\n",
    "\n",
    "_, p = mannwhitneyu(TPR_3, TPR_0, method=\"exact\")\n",
    "print(f\"XGBClassifier x DecisionTreeClassifier p-value: {p.round(5)}\")\n",
    "\n",
    "_, p = mannwhitneyu(TPR_3, TPR_1, method=\"exact\")\n",
    "print(f\"XGBClassifier x LogisticRegression p-value: {p.round(5)}\")\n",
    "\n",
    "_, p = mannwhitneyu(TPR_3, TPR_2, method=\"exact\")\n",
    "print(f\"XGBClassifier x RandomForestClassifier p-value: {p.round(5)}\")\n",
    "\n",
    "_, p = mannwhitneyu(TPR_3, TPR_4, method=\"exact\")\n",
    "print(f\"XGBClassifier x MLPClassifier p-value: {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selectt the metric to see the values\n",
    "metrics = [\"BAL_ACC\", \"PPV\", \"TPR\"]\n",
    "latex_acc, latex_ppv, latex_tpr, latex_f1 = [], [], [], []\n",
    "plot_acc, plot_ppv, plot_tpr, plot_f1 = [], [], [], []\n",
    "plot_acc_ci, plot_ppv_ci, plot_tpr_ci, plot_f1_ci = [], [], [], []\n",
    "\n",
    "for metric in metrics:\n",
    "    scores = df_scores.groupby([\"model_name\"])[[metric]]\n",
    "    means, accs, upper_ci, lower_ci, models = [], [], [], [], []\n",
    "\n",
    "    for model in scores.groups.keys():\n",
    "        models.append(model)\n",
    "        mean_acc = scores.get_group(model)[metric].mean()\n",
    "        means.append(mean_acc)\n",
    "        accs = scores.get_group(model)[metric]\n",
    "        ci = get_confidence_interval(accs)\n",
    "        lower_ci.append(np.abs(mean_acc - ci[0]))\n",
    "        upper_ci.append(np.abs(mean_acc - ci[1]))\n",
    "\n",
    "    for a, b in zip(means, lower_ci):\n",
    "        metric_str = f\"{a:.3f} $\\pm$ {b:.3f}\"\n",
    "\n",
    "        if metric == \"BAL_ACC\":\n",
    "            latex_acc.append(metric_str)\n",
    "            plot_acc.append(a.round(3))\n",
    "            plot_acc_ci.append(b.round(3))\n",
    "        elif metric == \"PPV\":\n",
    "            latex_ppv.append(metric_str)\n",
    "            plot_ppv.append(a.round(3))\n",
    "            plot_ppv_ci.append(b.round(3))\n",
    "        elif metric == \"TPR\":\n",
    "            latex_tpr.append(metric_str)\n",
    "            plot_tpr.append(a.round(3))\n",
    "            plot_tpr_ci.append(b.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Generic Metrics Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_generic_metrics.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selectt the metric to see the values\n",
    "metric = \"F1_MACRO\"\n",
    "\n",
    "generic_performance_scores = df_generic_metrics.groupby([\"model_name\"])[[metric]]\n",
    "means, accs, upper_ci, lower_ci, models = [], [], [], [], []\n",
    "plot_f1, plot_f1_ci = [], []\n",
    "\n",
    "for model in generic_performance_scores.groups.keys():\n",
    "    models.append(model)\n",
    "    mean_acc = generic_performance_scores.get_group(model)[metric].mean()\n",
    "    means.append(mean_acc)\n",
    "    accs = generic_performance_scores.get_group(model)[metric]\n",
    "    ci = get_confidence_interval(accs)\n",
    "    lower_ci.append(np.abs(mean_acc - ci[0]))\n",
    "    upper_ci.append(np.abs(mean_acc - ci[1]))\n",
    "\n",
    "\n",
    "for a, b in zip(means, lower_ci):\n",
    "    metric_str = f\"{a:.3f} $\\pm$ {b:.3f}\"\n",
    "    latex_f1.append(metric_str)\n",
    "    plot_f1.append(a.round(3))\n",
    "    plot_f1_ci.append(b.round(3))\n",
    "\n",
    "df = pd.DataFrame({\"model\": models, \"metric\": metric, \"mean\": means, \"pm_ci\": lower_ci})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"BAL_ACC\": latex_acc,\n",
    "    \"PPV\": latex_ppv,\n",
    "    \"TPR\": latex_tpr,\n",
    "    \"F1_MACRO\": latex_f1,\n",
    "}\n",
    "\n",
    "df_latex = pd.DataFrame(data, index=[\"DT\", \"LG\", \"NN\", \"RF\", \"XGB\"])\n",
    "\n",
    "# Convert DataFrame to LaTeX formatted table\n",
    "latex_table = df_latex.to_latex(escape=False)\n",
    "print(latex_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams[\"figure.dpi\"] = 100\n",
    "plt.rcParams[\"axes.labelsize\"] = 8\n",
    "plt.rcParams[\"lines.markersize\"] = 4\n",
    "plt.rcParams[\"ytick.labelsize\"] = 8\n",
    "\n",
    "colors = plt.get_cmap(\"Set1\")\n",
    "# colors = [c(1),c(6), c(10), c(14)]\n",
    "\n",
    "\n",
    "def plot_model_metrics(models, models_means, models_erros):\n",
    "    \"\"\"\n",
    "    Create a grouped bar chart of model metrics.\n",
    "\n",
    "    :param models: List of model names\n",
    "    :param models_means: Dictionary with metrics as keys and lists of values for each model as values\n",
    "    \"\"\"\n",
    "    # Set up the plot\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "    # Number of metrics and models\n",
    "    num_metrics = len(models_means)\n",
    "    num_models = len(models)\n",
    "\n",
    "    # Set width of each bar and positions of the bars\n",
    "    bar_width = 0.15\n",
    "    r = np.arange(num_models)\n",
    "\n",
    "    # Plot bars for each metric\n",
    "    for i, (metric, values) in enumerate(models_means.items()):\n",
    "        position = [x + bar_width * i for x in r]\n",
    "        plt.bar(\n",
    "            position, values, width=bar_width, label=metric, color=colors(i + 1), yerr=models_erros[metric], alpha=0.9\n",
    "        )\n",
    "\n",
    "    plt.xlabel(\"Models\")\n",
    "    plt.ylabel(\"Scores\")\n",
    "    plt.title(\"Training performance metrics over 10 cross-validation folds\", fontsize=10)\n",
    "    plt.xticks([r + bar_width * (num_metrics - 1) / 2 for r in range(num_models)], models)\n",
    "    # show grid x axis\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.6)\n",
    "    # Add legend\n",
    "    plt.legend(loc=\"upper center\", bbox_to_anchor=(0.5, 1.2), ncol=4, fontsize=9)\n",
    "\n",
    "    # Add value labels on the bars\n",
    "    # for i, (metric, values) in enumerate(models_means.items()):\n",
    "    #     for j, v in enumerate(values):\n",
    "    #         ax.text(r[j] + bar_width * i, v, f\"{v:.2f}\", ha=\"center\", va=\"bottom\")\n",
    "\n",
    "    # Adjust layout and display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"../assets/training_performance.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "models = [\"Decision Tree\", \"Log. Regression\", \"Neural Network\", \"Random Forest\", \"XGBoost\"]\n",
    "models_means = {\"bal_acc\": plot_acc, \"ppv\": plot_ppv, \"trp\": plot_tpr, \"f1_macro\": plot_f1}\n",
    "models_erros = {\"bal_acc\": plot_acc_ci, \"ppv\": plot_ppv_ci, \"trp\": plot_tpr_ci, \"f1_macro\": plot_f1_ci}\n",
    "plot_model_metrics(models, models_means, models_erros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
